{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Wall: COMP2420\n",
    "\n",
    "## Data Types\n",
    "\n",
    "- Nominal\n",
    "- Ordinal\n",
    "- Interval\n",
    "- Ratio\n",
    "\n",
    "\n",
    "## Basic plot types\n",
    "\n",
    "- 1D:\n",
    "- 2D: scatter, line plot, box and whisker, heatmap\n",
    "- 3D: scatter matrix, bubble chart\n",
    "\n",
    "## Machine Learning Notes\n",
    "\n",
    "- Improve on task T, with respect to performance metric P, based on experience E\n",
    "\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement learning\n",
    "\n",
    "\n",
    "## Something\n",
    "- We generally assume that the training and test examples are independently drawn from the same overall distribution of data\n",
    "    - We call this “i.i.d:” which stands for “independent and identically distributed”\n",
    "\n",
    "- Metrics on a dataset is what we care about (performance)\n",
    "- We typically cannot directly optimize for the metrics\n",
    "- Our loss function should reflect the problem we are solving. We then hope it will yield models that will do well on our dataset.\n",
    "\n",
    "## ML in Practice\n",
    "\n",
    "Generally:\n",
    "\n",
    "- We assume training and testing samples are independently drawn from the same overall distribution\n",
    "    - We call this IID, \"independent and identically distributed\"\n",
    "- Learning can either be:\n",
    "    - a) direct: learning from specific truths in the data\n",
    "    - b) indirect: learning from things inferred in the data\n",
    "\n",
    "We always try and make models for our datasets that are good fits, but how do we know what a good fit is?\n",
    "\n",
    "The residual sum of squares (RSS) is one such way:\n",
    "\n",
    "![rss](images/rss.svg)\n",
    "\n",
    "In this case y<sub>i</sub> represents the actual point, and x<sub>i</sub> the predicted point.\n",
    "\n",
    "So y<sub>i</sub> - x<sub>i</sub> is the difference between the actual and predicted data point, to negate the effect negatives might have on the average this is then squared.\n",
    "\n",
    "## Linear Models\n",
    "\n",
    "### What is Linear\n",
    "\n",
    "While up until this point our linear models have only been made up of linear equations (ie. `y = mx + b`), we can also make linear models out of polynomial equations (ie. `y = ax^2 + bx^3 + cx^4`). This is because when we talk about the \"linearity\" of a model, we're referring to the coefficients (in the last example `a, b, c`).\n",
    "\n",
    "### Determining Linear Relationships\n",
    "\n",
    "We can put a linear model on anything, but how do we know if there is actually a linear relationship in what we're observing?\n",
    "\n",
    "Approach: We use a hypothesis test. Null hypothesis is that there is no linear relationship.\n",
    "\n",
    "How do we measure it though? We need a value which is small under the null hypothesis, and large under the alternate hypothesis.\n",
    "\n",
    "Presenting, R^2...\n",
    "\n",
    "![r2](images/r_2.png)\n",
    "\n",
    "Notice RSS on top there!\n",
    "\n",
    "In this syntax y<sub>i</sub> is the actual value, again, and y bar is the mean y value.\n",
    "\n",
    "If R^2 is 1, that means that all of the variance of y is explained in the prediction. If it is zero that means none of the variance is explained.\n",
    "\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "How do you chose attributes to base a decision tree on?\n",
    "\n",
    "- Deterministic attributes: good (all are true or all are false)\n",
    "- Uniform attributes: bad (all classes of leaf are equally probable)\n",
    "\n",
    "_Entropy_\n",
    "\n",
    "Can be calculated with `scipi.stats.entropy`.\n",
    "\n",
    "### What makes a good tree?\n",
    "\n",
    "- Not too small\n",
    "    - Needs to handle subtle distinctions between data\n",
    "- Not too big\n",
    "    - Computational efficiency will be compromied (avoid redundant attribute, etc.)\n",
    "    - Avoid over fitting of training examples\n",
    "\n",
    "Follow Occam's razor: find the simplest hypothesis that fits all observations. Doing this will lead to smaller trees with more informative nodes near the root.\n",
    "\n",
    "Best case scenario: when a classification depends critically on only a few attributes\n",
    "\n",
    "Worst case scenario: parity, majority functions. also bad for continuous attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
